{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textcnn for colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serereuk/textcnn-for-colab/blob/master/textcnn_for_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uHCo6b8LA9Ye",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Text CNN 추천 수 가중치 계산용"
      ]
    },
    {
      "metadata": {
        "id": "YVJJyebrF-9e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###장점 : 잘됨  \n",
        "\n",
        "###단점 : 왜 잘되는지 몰겠음  \n",
        "\n",
        "###앞으로 해야하는 일:\n",
        "1. 이 친구 시각화해서 보여주기 텐서 그래프 사용!\n"
      ]
    },
    {
      "metadata": {
        "id": "QIwtsq3q-Quu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ofe3eV55-Suu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YtZRxUm3-SxS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x5MXmG9S-S0r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "download = drive.CreateFile({\"id\": \"1mrfmLq7mdc8NTMdLMUIOBLNJyX3K4CV8\"}) #이부분 수정하면 됩니당\n",
        "download.GetContentFile(\"month_tokenize_data.pickle\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5TFPSvC9-lQy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"month_tokenize_data.pickle\",\"rb\") as f:\n",
        "  data = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dFM7lib9BUk7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1w2No9ESBuPU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##Text CNN 파라메터 소개  \n",
        "\n",
        "\n",
        "*   sequence_length : x의 column 및 최대 단어 개수 \n",
        "*   num_classes : y의 차원, binary 인지 multinomial인지\n",
        "*   vocab_size : 단어의 총 개수\n",
        "*   embedding_size : 임배딩 차원\n",
        "*   filter_sizes: 필터 사이즈 여기선 [3,4,5]로 함\n",
        "*   num_filters : # of filters, 여기선 128개였던걸로 기억\n",
        "*   l2_reg_lambda: Regularization 2 람다값\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "49dwyVQM-S2h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class textcnn():\n",
        "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
        "                 embedding_size, filter_sizes, num_filters, l2_reg_lambda = 0.0):\n",
        "      \n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length],name = \"x\") # x\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes],name = \"y\") # y\n",
        "        self.dropout_keep_prob=tf.placeholder(tf.float32, name=\"dropout\") #드랍아웃\n",
        "        \n",
        "\n",
        "        l2_loss = tf.constant(0.0) # 로스2 초기화\n",
        "\n",
        "        \n",
        "        with tf.name_scope(\"embedding\"): # 시각화를 위한 네임 스코프 여기서 하는건 임베딩 친구들\n",
        "            W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name = \"W\")\n",
        "            # 가중치 친구 더블유 우리가 설정한 차원으로 보내준다\n",
        "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x) # 진짜 임배딩 하는중\n",
        "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "\n",
        "            \n",
        "        pooled_outputs= [] # 결과물 리스트\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            # 필터 사이즈가 리스트로 들어갔기 때문에 이와 같은 enumerate 구문을 사용 \n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size): # 시각화를 위한 네임스코프 \n",
        "                filter_shape = [filter_size, embedding_size, 1, num_filters] # 필터 차원 [3,128,1,128]\n",
        "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev= 0.1), name= \"W\") #웨이트 초기화 절사정규분포로 \n",
        "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name= \"b\") # 0.1로 바이어스 초기화\n",
        "                conv = tf.nn.conv2d(self.embedded_chars_expanded, #콘볼루션 친구 귀엽게 생겼다.\n",
        "                                    W,\n",
        "                                    strides = [1,1,1,1],\n",
        "                                    padding=\"VALID\",\n",
        "                                    name=\"conv\")\n",
        "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name = \"relu\") #렐루렐루\n",
        "                pooled = tf.nn.max_pool(h,\n",
        "                                        ksize = [1, sequence_length - filter_size + 1, 1, 1],\n",
        "                                        strides= [1,1,1,1],\n",
        "                                        padding=\"VALID\",\n",
        "                                        name= \"pool\") #cnn의 특징 중 하나인 맥스풀을 사용하는 모습\n",
        "\n",
        "                pooled_outputs.append(pooled) # 레이어들을 이어주자\n",
        "                \n",
        "                \n",
        "        num_filters_total = num_filters * len(filter_sizes) # 총 필터 개수는 3,4,5 모두 각각 128개니까 128*3\n",
        "        #print(pooled_outputs)\n",
        "        self.h_pool = tf.concat(pooled_outputs,3) # 오늘의 리빙 포인트 tf.concat이 파라메터 넣는게 바뀌었다.\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total]) # 펼치기\n",
        "\n",
        "        with tf.name_scope(\"dropout\"): # 드랍아웃 네임스코프\n",
        "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
        "            \n",
        "        with tf.name_scope(\"output\"): # 마지막 레이어 부분에 알맞은 차원으로 나오게 만들어주는 모습\n",
        "            W = tf.get_variable(\"W\", shape = [num_filters_total, num_classes]\n",
        "                                , initializer=tf.contrib.layers.xavier_initializer()) # 자비어로 초기화\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name= \"b\")\n",
        "            l2_loss += tf.nn.l2_loss(W) # l2 로스\n",
        "            l2_loss += tf.nn.l2_loss(b)\n",
        "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"score\") # 확률\n",
        "            self.prediction = tf.argmax(self.scores, 1, name=\"prediction\") # 예측\n",
        "\n",
        "        with tf.name_scope(\"loss\"): # 로스 친구는 크로스엔트로피 소프트 멕스 + l2 regularization\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits(labels = self.input_y, logits= self.scores)\n",
        "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "        with tf.name_scope(\"accuracy\"): # 정확도 친구\n",
        "            correct_prediction = tf.equal(self.prediction, tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"), name = \"accuracy\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RlqyG0wmHxTc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##전처리 부분:\n",
        "\n",
        "1.   음절을 잘랐다. 예를 들면 '안녕하세요 집에 가고 싶어요' 같은 경우 3음절로 잘라서 진행 '안녕하', '집에', '가고', '싶어요' 로 학습 시킴'\n",
        "2.   tf.contrib.learn.preprocessing.VocabularyProcessor 사용\n",
        "3.   예측변수 y에 대해서는 추천수가 100 이하면 [1,0] 초과면 [0,1] 으로 둠\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yB_W6p5N-S5J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "def cut(contents, cut= 3):\n",
        "    results = []\n",
        "    for content in contents:\n",
        "        words = content.split()\n",
        "        result = []\n",
        "        for word in words:\n",
        "            result.append(word[:cut])\n",
        "\n",
        "        results.append(\"\".join([token for token in result]))\n",
        "    return results\n",
        "\n",
        "def make_input(documents, max_document):\n",
        "    vocab = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document)\n",
        "    x = np.array(list(vocab.fit_transform(documents)))\n",
        "    vocab_dict = vocab.vocabulary_._mapping\n",
        "    sorted_vocab = sorted(vocab_dict.items(), key = lambda x: x[1])\n",
        "    vocabulary = list(list(zip(*sorted_vocab))[0])\n",
        "    return x, vocabulary, len(vocab.vocabulary_)\n",
        "\n",
        "def make_output(points, threshold):\n",
        "    results = np.zeros((len(points),2))\n",
        "    for idx, point in enumerate(points):\n",
        "        if point > threshold:\n",
        "            results[idx,0] = 1\n",
        "        else:\n",
        "            results[idx,1] = 1\n",
        "    return results\n",
        "\n",
        "def check_maxlength(contents):\n",
        "    max_document_length = 0\n",
        "    for document in contents:\n",
        "        document_length = len(document.split())\n",
        "        if document_length > max_document_length:\n",
        "            max_document_length = document_length\n",
        "    return max_document_length\n",
        "  \n",
        "def divide(x, y, train_prop):\n",
        "    random.seed(1234)\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    tmp = np.random.permutation(np.arange(len(x)))\n",
        "    x_tr = x[tmp][:round(train_prop * len(x))]\n",
        "    y_tr = y[tmp][:round(train_prop * len(x))]\n",
        "    x_te = x[tmp][-(len(x)-round(train_prop * len(x))):]\n",
        "    y_te = y[tmp][-(len(x)-round(train_prop * len(x))):]\n",
        "    return x_tr, x_te, y_tr, y_te\n",
        "\n",
        "\n",
        "x, vocab, vocab_size = make_input(data['content'], 200)\n",
        "y = make_output(list(map(int,[x.replace(\",\",\"\") for x in data['count']])), 100)\n",
        "x_train, x_test, y_train, y_test = divide(x,y,train_prop=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m9L25mkRI9Im",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##FLAGS 사용  \n",
        "####장점 : 상수처럼 사용 가능\n",
        "####단점 : 아주 불편하다... 텐서 버젼을 잘 확인하고 다녀야한다.\n",
        "####단점 : 두번 설정(선언) 불가능, 한 번 잘못 선언하면 파이썬을 끄던지 그 변수를 삭제하고 진행해야한다."
      ]
    },
    {
      "metadata": {
        "id": "fJDTbX3e2ztU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel') # 이 문장이 핵심 포인트... 이거 없으면 오류만 난다.\n",
        "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"for embedding\")\n",
        "tf.flags.DEFINE_string(\"filter_size\", \"3,4,5\", \"filter_size\")\n",
        "tf.flags.DEFINE_integer(\"num_filters\", 128, \"number of filter\")\n",
        "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"for dropout\")\n",
        "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"lambda\")\n",
        "tf.flags.DEFINE_integer(\"batch_size\", 64, \"batchsize\")\n",
        "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"number of training\")\n",
        "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate\")\n",
        "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"save\")\n",
        "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"number of checkpoints\")\n",
        "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"allow soft device\")\n",
        "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"log placement\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8RnrxdjTJhRh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##학습 과정\n",
        "\n",
        "###ratsgo를 많이 참고하였다."
      ]
    },
    {
      "metadata": {
        "id": "G65jic13-_QN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with tf.Graph().as_default():\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        cnn = textcnn(sequence_length=x_train.shape[1],\n",
        "                      num_classes=y_train.shape[1],\n",
        "                      vocab_size=vocab_size,\n",
        "                      embedding_size=FLAGS.embedding_dim,\n",
        "                      filter_sizes=list(map(int, FLAGS.filter_size.split(\",\"))),\n",
        "                      num_filters=FLAGS.num_filters,\n",
        "                      l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
        "\n",
        "        # Define Training procedure\n",
        "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
        "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "        # Keep track of gradient values and sparsity (optional)\n",
        "        grad_summaries = []\n",
        "        for g, v in grads_and_vars:\n",
        "            if g is not None:\n",
        "                grad_hist_summary = tf.summary.histogram(\"{}\".format(v.name), g)\n",
        "                sparsity_summary = tf.summary.scalar(\"{}\".format(v.name), tf.nn.zero_fraction(g))\n",
        "                grad_summaries.append(grad_hist_summary)\n",
        "                grad_summaries.append(sparsity_summary)\n",
        "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
        "\n",
        "        # Output directory for models and summaries\n",
        "        timestamp = str(int(time.time()))\n",
        "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "        print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "        # Summaries for loss and accuracy\n",
        "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
        "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
        "\n",
        "        # Train Summaries\n",
        "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
        "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "        # Dev summaries\n",
        "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
        "\n",
        "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
        "\n",
        "        # Initialize all variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        def train_step(x_batch, y_batch):\n",
        "            \"\"\"\n",
        "            A single training step\n",
        "            \"\"\"\n",
        "            feed_dict = {\n",
        "                cnn.input_x: x_batch,\n",
        "                cnn.input_y: y_batch,\n",
        "                cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
        "            }\n",
        "            _, step, summaries, loss, accuracy = sess.run(\n",
        "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            train_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "\n",
        "        def dev_step(x_batch, y_batch, writer=None):\n",
        "            \"\"\"\n",
        "            Evaluates model on a dev set\n",
        "            \"\"\"\n",
        "            feed_dict = {\n",
        "                cnn.input_x: x_batch,\n",
        "                cnn.input_y: y_batch,\n",
        "                cnn.dropout_keep_prob: 1.0\n",
        "            }\n",
        "            step, summaries, loss, accuracy = sess.run(\n",
        "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            if writer:\n",
        "                writer.add_summary(summaries, step)\n",
        "\n",
        "\n",
        "        def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
        "            \"\"\"\n",
        "            Generates a batch iterator for a dataset.\n",
        "            \"\"\"\n",
        "            data = np.array(data)\n",
        "            data_size = len(data)\n",
        "            num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
        "            for epoch in range(num_epochs):\n",
        "                # Shuffle the data at each epoch\n",
        "                if shuffle:\n",
        "                    shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "                    shuffled_data = data[shuffle_indices]\n",
        "                else:\n",
        "                    shuffled_data = data\n",
        "                for batch_num in range(num_batches_per_epoch):\n",
        "                    start_index = batch_num * batch_size\n",
        "                    end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "                    yield shuffled_data[start_index:end_index]\n",
        "\n",
        "\n",
        "        # Generate batches\n",
        "        batches = batch_iter(\n",
        "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
        "\n",
        "        testpoint = 0\n",
        "        # Training loop. For each batch...\n",
        "        for batch in batches:\n",
        "            x_batch, y_batch = zip(*batch)\n",
        "            train_step(x_batch, y_batch)\n",
        "            current_step = tf.train.global_step(sess, global_step)\n",
        "            if current_step % FLAGS.evaluate_every == 0:\n",
        "                if testpoint + 100 < len(x_test):\n",
        "                    testpoint += 100\n",
        "                else:\n",
        "                    testpoint = 0\n",
        "                print(\"\\nEvaluation:\")\n",
        "                dev_step(x_test[testpoint:testpoint+100], y_test[testpoint:testpoint+100], writer=dev_summary_writer)\n",
        "                print(\"\")\n",
        "            if current_step % FLAGS.checkpoint_every == 0:\n",
        "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "                print(\"Saved model checkpoint to {}\\n\".format(path))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}