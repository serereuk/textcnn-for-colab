{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textcnn for colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serereuk/textcnn-for-colab/blob/master/textcnn_for_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "QIwtsq3q-Quu",
        "colab_type": "code",
        "outputId": "35033e86-72bd-4692-d1ac-6bf6342a0938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyDrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 19.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.11.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.4)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.2)\n",
            "Building wheels for collected packages: PyDrive\n",
            "  Running setup.py bdist_wheel for PyDrive ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built PyDrive\n",
            "Installing collected packages: PyDrive\n",
            "Successfully installed PyDrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ofe3eV55-Suu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YtZRxUm3-SxS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x5MXmG9S-S0r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "download = drive.CreateFile({\"id\": \"1mrfmLq7mdc8NTMdLMUIOBLNJyX3K4CV8\"}) #이부분 수정하면 됩니당\n",
        "download.GetContentFile(\"month_tokenize_data.pickle\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5TFPSvC9-lQy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"month_tokenize_data.pickle\",\"rb\") as f:\n",
        "  data = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "49dwyVQM-S2h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class textcnn():\n",
        "    def __init__(self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters,\n",
        "                 l2_reg_lambda = 0.0):\n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name = \"x\")\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name = \"y\")\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout\")\n",
        "\n",
        "        l2_loss = tf.constant(0.0)\n",
        "\n",
        "        with tf.name_scope(\"embedding\"):\n",
        "            W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name = \"W\")\n",
        "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
        "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "\n",
        "        pooled_outputs= []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
        "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev= 0.1), name= \"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name= \"b\")\n",
        "                conv = tf.nn.conv2d(self.embedded_chars_expanded,\n",
        "                                    W,\n",
        "                                    strides = [1,1,1,1],\n",
        "                                    padding=\"VALID\",\n",
        "                                    name=\"conv\")\n",
        "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name = \"relu\")\n",
        "                pooled = tf.nn.max_pool(h,\n",
        "                                        ksize = [1, sequence_length - filter_size + 1, 1, 1],\n",
        "                                        strides= [1,1,1,1],\n",
        "                                        padding=\"VALID\",\n",
        "                                        name= \"pool\")\n",
        "\n",
        "                pooled_outputs.append(pooled)\n",
        "        num_filters_total = num_filters * len(filter_sizes)\n",
        "        #print(pooled_outputs)\n",
        "        self.h_pool = tf.concat(pooled_outputs,3)\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
        "            \n",
        "        with tf.name_scope(\"output\"):\n",
        "            W = tf.get_variable(\"W\", shape = [num_filters_total, num_classes]\n",
        "                                , initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name= \"b\")\n",
        "            l2_loss += tf.nn.l2_loss(W)\n",
        "            l2_loss += tf.nn.l2_loss(b)\n",
        "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"score\")\n",
        "            self.prediction = tf.argmax(self.scores, 1, name=\"prediction\")\n",
        "\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits(labels = self.input_y, logits= self.scores)\n",
        "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            correct_prediction = tf.equal(self.prediction, tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"), name = \"accuracy\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yB_W6p5N-S5J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "def cut(contents, cut= 3):\n",
        "    results = []\n",
        "    for content in contents:\n",
        "        words = content.split()\n",
        "        result = []\n",
        "        for word in words:\n",
        "            result.append(word[:cut])\n",
        "\n",
        "        results.append(\"\".join([token for token in result]))\n",
        "    return results\n",
        "\n",
        "def make_input(documents, max_document):\n",
        "    vocab = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document)\n",
        "    x = np.array(list(vocab.fit_transform(documents)))\n",
        "    vocab_dict = vocab.vocabulary_._mapping\n",
        "    sorted_vocab = sorted(vocab_dict.items(), key = lambda x: x[1])\n",
        "    vocabulary = list(list(zip(*sorted_vocab))[0])\n",
        "    return x, vocabulary, len(vocab.vocabulary_)\n",
        "\n",
        "def make_output(points, threshold):\n",
        "    results = np.zeros((len(points),2))\n",
        "    for idx, point in enumerate(points):\n",
        "        if point > threshold:\n",
        "            results[idx,0] = 1\n",
        "        else:\n",
        "            results[idx,1] = 1\n",
        "    return results\n",
        "\n",
        "def check_maxlength(contents):\n",
        "    max_document_length = 0\n",
        "    for document in contents:\n",
        "        document_length = len(document.split())\n",
        "        if document_length > max_document_length:\n",
        "            max_document_length = document_length\n",
        "    return max_document_length\n",
        "  \n",
        "def divide(x, y, train_prop):\n",
        "    random.seed(1234)\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    tmp = np.random.permutation(np.arange(len(x)))\n",
        "    x_tr = x[tmp][:round(train_prop * len(x))]\n",
        "    y_tr = y[tmp][:round(train_prop * len(x))]\n",
        "    x_te = x[tmp][-(len(x)-round(train_prop * len(x))):]\n",
        "    y_te = y[tmp][-(len(x)-round(train_prop * len(x))):]\n",
        "    return x_tr, x_te, y_tr, y_te\n",
        "\n",
        "\n",
        "x, vocab, vocab_size = make_input(data['content'], 200)\n",
        "y = make_output(list(map(int,[x.replace(\",\",\"\") for x in data['count']])), 100)\n",
        "x_train, x_test, y_train, y_test = divide(x,y,train_prop=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fJDTbX3e2ztU",
        "colab_type": "code",
        "outputId": "65885155-dd0b-47ac-d7a0-64c87abfb54e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4179
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"for embedding\")\n",
        "tf.flags.DEFINE_string(\"filter_size\", \"3,4,5\", \"filter_size\")\n",
        "tf.flags.DEFINE_integer(\"num_filters\", 128, \"number of filter\")\n",
        "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"for dropout\")\n",
        "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"lambda\")\n",
        "tf.flags.DEFINE_integer(\"batch_size\", 64, \"batchsize\")\n",
        "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"number of training\")\n",
        "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate\")\n",
        "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"save\")\n",
        "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"number of checkpoints\")\n",
        "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"allow soft device\")\n",
        "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"log placement\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name embedding/W:0 is illegal; using embedding/W_0 instead.\n",
            "INFO:tensorflow:Summary name embedding/W:0 is illegal; using embedding/W_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/W:0 is illegal; using conv-maxpool-3/W_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/W:0 is illegal; using conv-maxpool-3/W_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/b:0 is illegal; using conv-maxpool-3/b_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/b:0 is illegal; using conv-maxpool-3/b_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/W:0 is illegal; using conv-maxpool-4/W_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/W:0 is illegal; using conv-maxpool-4/W_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/b:0 is illegal; using conv-maxpool-4/b_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/b:0 is illegal; using conv-maxpool-4/b_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/W:0 is illegal; using conv-maxpool-5/W_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/W:0 is illegal; using conv-maxpool-5/W_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/b:0 is illegal; using conv-maxpool-5/b_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/b:0 is illegal; using conv-maxpool-5/b_0 instead.\n",
            "INFO:tensorflow:Summary name W:0 is illegal; using W_0 instead.\n",
            "INFO:tensorflow:Summary name W:0 is illegal; using W_0 instead.\n",
            "INFO:tensorflow:Summary name output/b:0 is illegal; using output/b_0 instead.\n",
            "INFO:tensorflow:Summary name output/b:0 is illegal; using output/b_0 instead.\n",
            "Writing to /content/runs/1546238525\n",
            "\n",
            "2018-12-31T06:42:09.813625: step 1, loss 2.3448, acc 0.515625\n",
            "2018-12-31T06:42:10.370004: step 2, loss 1.05519, acc 0.71875\n",
            "2018-12-31T06:42:10.934020: step 3, loss 0.652571, acc 0.890625\n",
            "2018-12-31T06:42:11.484554: step 4, loss 0.99087, acc 0.890625\n",
            "2018-12-31T06:42:12.051822: step 5, loss 0.367053, acc 0.96875\n",
            "2018-12-31T06:42:12.604855: step 6, loss 0.411079, acc 0.953125\n",
            "2018-12-31T06:42:13.171497: step 7, loss 0.423993, acc 0.953125\n",
            "2018-12-31T06:42:13.730803: step 8, loss 0.304639, acc 0.96875\n",
            "2018-12-31T06:42:14.292648: step 9, loss 0.776014, acc 0.953125\n",
            "2018-12-31T06:42:14.863172: step 10, loss 0.781925, acc 0.96875\n",
            "2018-12-31T06:42:15.419455: step 11, loss 0.930424, acc 0.96875\n",
            "2018-12-31T06:42:15.980796: step 12, loss 0.622534, acc 0.96875\n",
            "2018-12-31T06:42:16.534983: step 13, loss 0.90717, acc 0.96875\n",
            "2018-12-31T06:42:17.095661: step 14, loss 0.579835, acc 0.984375\n",
            "2018-12-31T06:42:17.656309: step 15, loss 0.642631, acc 0.984375\n",
            "2018-12-31T06:42:18.218630: step 16, loss 0.59844, acc 0.984375\n",
            "2018-12-31T06:42:18.771573: step 17, loss 0.193042, acc 1\n",
            "2018-12-31T06:42:19.330822: step 18, loss 1.40154, acc 0.9375\n",
            "2018-12-31T06:42:19.883619: step 19, loss 1.51425, acc 0.9375\n",
            "2018-12-31T06:42:20.439884: step 20, loss 0.49686, acc 0.96875\n",
            "2018-12-31T06:42:20.996219: step 21, loss 0.192142, acc 1\n",
            "2018-12-31T06:42:21.551989: step 22, loss 0.95556, acc 0.96875\n",
            "2018-12-31T06:42:22.117493: step 23, loss 0.192978, acc 1\n",
            "2018-12-31T06:42:22.671289: step 24, loss 0.191257, acc 1\n",
            "2018-12-31T06:42:23.231280: step 25, loss 1.24034, acc 0.953125\n",
            "2018-12-31T06:42:23.787426: step 26, loss 0.772485, acc 0.96875\n",
            "2018-12-31T06:42:24.352196: step 27, loss 0.671352, acc 0.96875\n",
            "2018-12-31T06:42:24.911093: step 28, loss 0.924611, acc 0.953125\n",
            "2018-12-31T06:42:25.480814: step 29, loss 0.870961, acc 0.9375\n",
            "2018-12-31T06:42:26.046327: step 30, loss 1.24947, acc 0.953125\n",
            "2018-12-31T06:42:26.616253: step 31, loss 0.555134, acc 0.96875\n",
            "2018-12-31T06:42:27.178397: step 32, loss 1.08373, acc 0.9375\n",
            "2018-12-31T06:42:27.741589: step 33, loss 1.38153, acc 0.921875\n",
            "2018-12-31T06:42:28.311013: step 34, loss 0.73954, acc 0.9375\n",
            "2018-12-31T06:42:28.876363: step 35, loss 0.196518, acc 1\n",
            "2018-12-31T06:42:29.443463: step 36, loss 0.592823, acc 0.9375\n",
            "2018-12-31T06:42:30.010653: step 37, loss 0.756838, acc 0.90625\n",
            "2018-12-31T06:42:30.584520: step 38, loss 0.779412, acc 0.921875\n",
            "2018-12-31T06:42:31.155690: step 39, loss 0.788583, acc 0.890625\n",
            "2018-12-31T06:42:31.732406: step 40, loss 0.91593, acc 0.90625\n",
            "2018-12-31T06:42:32.306065: step 41, loss 0.646229, acc 0.890625\n",
            "2018-12-31T06:42:32.875777: step 42, loss 0.623738, acc 0.859375\n",
            "2018-12-31T06:42:33.445878: step 43, loss 0.348793, acc 0.9375\n",
            "2018-12-31T06:42:34.015132: step 44, loss 0.688088, acc 0.84375\n",
            "2018-12-31T06:42:34.586056: step 45, loss 0.368353, acc 0.9375\n",
            "2018-12-31T06:42:35.148754: step 46, loss 0.950033, acc 0.875\n",
            "2018-12-31T06:42:35.718329: step 47, loss 1.05069, acc 0.890625\n",
            "2018-12-31T06:42:36.281439: step 48, loss 0.665791, acc 0.90625\n",
            "2018-12-31T06:42:36.855927: step 49, loss 0.8078, acc 0.875\n",
            "2018-12-31T06:42:37.428246: step 50, loss 0.371627, acc 0.953125\n",
            "2018-12-31T06:42:37.998011: step 51, loss 0.870829, acc 0.953125\n",
            "2018-12-31T06:42:38.569722: step 52, loss 0.966001, acc 0.921875\n",
            "2018-12-31T06:42:39.136987: step 53, loss 0.5718, acc 0.953125\n",
            "2018-12-31T06:42:39.710400: step 54, loss 0.599835, acc 0.9375\n",
            "2018-12-31T06:42:40.277995: step 55, loss 0.198112, acc 1\n",
            "2018-12-31T06:42:40.850194: step 56, loss 0.867481, acc 0.9375\n",
            "2018-12-31T06:42:41.402170: step 57, loss 0.88327, acc 0.9375\n",
            "2018-12-31T06:42:41.979980: step 58, loss 0.558411, acc 0.953125\n",
            "2018-12-31T06:42:42.552083: step 59, loss 0.68074, acc 0.953125\n",
            "2018-12-31T06:42:43.114399: step 60, loss 0.354284, acc 0.96875\n",
            "2018-12-31T06:42:43.689010: step 61, loss 1.18773, acc 0.921875\n",
            "2018-12-31T06:42:44.251514: step 62, loss 0.207432, acc 0.984375\n",
            "2018-12-31T06:42:44.830789: step 63, loss 0.748245, acc 0.9375\n",
            "2018-12-31T06:42:45.392302: step 64, loss 0.726636, acc 0.9375\n",
            "2018-12-31T06:42:45.963484: step 65, loss 0.598069, acc 0.9375\n",
            "2018-12-31T06:42:46.526117: step 66, loss 0.52265, acc 0.953125\n",
            "2018-12-31T06:42:47.099581: step 67, loss 0.707424, acc 0.9375\n",
            "2018-12-31T06:42:47.675667: step 68, loss 0.690703, acc 0.9375\n",
            "2018-12-31T06:42:48.241740: step 69, loss 0.387713, acc 0.96875\n",
            "2018-12-31T06:42:48.816873: step 70, loss 0.452286, acc 0.921875\n",
            "2018-12-31T06:42:49.379657: step 71, loss 0.738894, acc 0.9375\n",
            "2018-12-31T06:42:49.951231: step 72, loss 0.446563, acc 0.96875\n",
            "2018-12-31T06:42:50.518962: step 73, loss 0.29937, acc 0.921875\n",
            "2018-12-31T06:42:51.094115: step 74, loss 0.947442, acc 0.90625\n",
            "2018-12-31T06:42:51.664090: step 75, loss 0.741141, acc 0.921875\n",
            "2018-12-31T06:42:52.244928: step 76, loss 0.905975, acc 0.859375\n",
            "2018-12-31T06:42:52.818198: step 77, loss 0.846722, acc 0.921875\n",
            "2018-12-31T06:42:53.386275: step 78, loss 0.561535, acc 0.890625\n",
            "2018-12-31T06:42:53.966686: step 79, loss 0.472663, acc 0.9375\n",
            "2018-12-31T06:42:54.527795: step 80, loss 0.219536, acc 0.96875\n",
            "2018-12-31T06:42:55.091614: step 81, loss 0.53386, acc 0.90625\n",
            "2018-12-31T06:42:55.645935: step 82, loss 0.4498, acc 0.953125\n",
            "2018-12-31T06:42:56.208889: step 83, loss 0.180294, acc 1\n",
            "2018-12-31T06:42:56.766641: step 84, loss 0.380823, acc 0.953125\n",
            "2018-12-31T06:42:57.333152: step 85, loss 0.906171, acc 0.921875\n",
            "2018-12-31T06:42:57.887041: step 86, loss 0.600386, acc 0.96875\n",
            "2018-12-31T06:42:58.439502: step 87, loss 0.381977, acc 0.96875\n",
            "2018-12-31T06:42:59.746333: step 88, loss 0.725745, acc 0.969697\n",
            "2018-12-31T06:43:00.302099: step 89, loss 0.358815, acc 0.953125\n",
            "2018-12-31T06:43:00.849299: step 90, loss 0.801762, acc 0.9375\n",
            "2018-12-31T06:43:01.401982: step 91, loss 0.361179, acc 0.96875\n",
            "2018-12-31T06:43:01.950504: step 92, loss 0.38099, acc 0.984375\n",
            "2018-12-31T06:43:02.522822: step 93, loss 0.282424, acc 0.96875\n",
            "2018-12-31T06:43:03.091191: step 94, loss 0.18953, acc 0.984375\n",
            "2018-12-31T06:43:03.659012: step 95, loss 0.564993, acc 0.9375\n",
            "2018-12-31T06:43:04.232568: step 96, loss 0.440527, acc 0.96875\n",
            "2018-12-31T06:43:04.814768: step 97, loss 0.391457, acc 0.953125\n",
            "2018-12-31T06:43:05.388495: step 98, loss 0.303671, acc 0.953125\n",
            "2018-12-31T06:43:05.956469: step 99, loss 0.393128, acc 0.9375\n",
            "2018-12-31T06:43:06.534045: step 100, loss 1.04349, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2018-12-31T06:43:06.619838: step 100, loss 0.358733, acc 0.96\n",
            "\n",
            "Saved model checkpoint to /content/runs/1546238525/checkpoints/model-100\n",
            "\n",
            "2018-12-31T06:43:09.341386: step 101, loss 0.528582, acc 0.90625\n",
            "2018-12-31T06:43:09.910445: step 102, loss 0.368748, acc 0.890625\n",
            "2018-12-31T06:43:10.482708: step 103, loss 0.915481, acc 0.828125\n",
            "2018-12-31T06:43:11.049033: step 104, loss 0.597432, acc 0.9375\n",
            "2018-12-31T06:43:11.623732: step 105, loss 0.339354, acc 0.953125\n",
            "2018-12-31T06:43:12.195247: step 106, loss 0.537999, acc 0.953125\n",
            "2018-12-31T06:43:12.763858: step 107, loss 0.674748, acc 0.921875\n",
            "2018-12-31T06:43:13.325505: step 108, loss 0.395409, acc 0.9375\n",
            "2018-12-31T06:43:13.893620: step 109, loss 0.381475, acc 0.96875\n",
            "2018-12-31T06:43:14.463765: step 110, loss 1.12151, acc 0.890625\n",
            "2018-12-31T06:43:15.028777: step 111, loss 0.376498, acc 0.890625\n",
            "2018-12-31T06:43:15.593558: step 112, loss 0.282235, acc 0.96875\n",
            "2018-12-31T06:43:16.159260: step 113, loss 0.528908, acc 0.921875\n",
            "2018-12-31T06:43:16.731762: step 114, loss 0.182089, acc 0.984375\n",
            "2018-12-31T06:43:17.292544: step 115, loss 0.310815, acc 0.953125\n",
            "2018-12-31T06:43:17.869916: step 116, loss 0.431265, acc 0.953125\n",
            "2018-12-31T06:43:18.430051: step 117, loss 0.825224, acc 0.890625\n",
            "2018-12-31T06:43:18.992910: step 118, loss 0.369336, acc 0.984375\n",
            "2018-12-31T06:43:19.561814: step 119, loss 0.41208, acc 0.984375\n",
            "2018-12-31T06:43:20.125481: step 120, loss 0.519174, acc 0.96875\n",
            "2018-12-31T06:43:20.699483: step 121, loss 0.269742, acc 0.96875\n",
            "2018-12-31T06:43:21.259315: step 122, loss 0.618139, acc 0.921875\n",
            "2018-12-31T06:43:21.827440: step 123, loss 0.486057, acc 0.9375\n",
            "2018-12-31T06:43:22.387028: step 124, loss 0.218523, acc 0.96875\n",
            "2018-12-31T06:43:22.958577: step 125, loss 0.312904, acc 0.953125\n",
            "2018-12-31T06:43:23.524265: step 126, loss 0.701341, acc 0.90625\n",
            "2018-12-31T06:43:24.089845: step 127, loss 0.469322, acc 0.9375\n",
            "2018-12-31T06:43:24.661521: step 128, loss 0.657642, acc 0.890625\n",
            "2018-12-31T06:43:25.225365: step 129, loss 0.344737, acc 0.96875\n",
            "2018-12-31T06:43:25.800085: step 130, loss 0.639486, acc 0.859375\n",
            "2018-12-31T06:43:26.363876: step 131, loss 0.559782, acc 0.96875\n",
            "2018-12-31T06:43:26.935146: step 132, loss 0.534716, acc 0.921875\n",
            "2018-12-31T06:43:27.502229: step 133, loss 0.633549, acc 0.90625\n",
            "2018-12-31T06:43:28.079670: step 134, loss 0.324334, acc 0.96875\n",
            "2018-12-31T06:43:28.649244: step 135, loss 0.424192, acc 0.921875\n",
            "2018-12-31T06:43:29.212765: step 136, loss 0.283639, acc 0.9375\n",
            "2018-12-31T06:43:29.784429: step 137, loss 0.235879, acc 0.953125\n",
            "2018-12-31T06:43:30.350603: step 138, loss 0.814664, acc 0.921875\n",
            "2018-12-31T06:43:30.922047: step 139, loss 0.473195, acc 0.96875\n",
            "2018-12-31T06:43:31.484789: step 140, loss 0.165766, acc 1\n",
            "2018-12-31T06:43:32.056106: step 141, loss 0.266305, acc 0.96875\n",
            "2018-12-31T06:43:32.622948: step 142, loss 0.366937, acc 0.96875\n",
            "2018-12-31T06:43:33.197991: step 143, loss 0.356496, acc 0.953125\n",
            "2018-12-31T06:43:33.764067: step 144, loss 0.163873, acc 1\n",
            "2018-12-31T06:43:34.339862: step 145, loss 0.375994, acc 0.953125\n",
            "2018-12-31T06:43:34.915017: step 146, loss 0.440597, acc 0.9375\n",
            "2018-12-31T06:43:35.481629: step 147, loss 0.967182, acc 0.90625\n",
            "2018-12-31T06:43:36.058340: step 148, loss 0.180718, acc 0.984375\n",
            "2018-12-31T06:43:36.625431: step 149, loss 0.271601, acc 0.96875\n",
            "2018-12-31T06:43:37.198189: step 150, loss 0.281713, acc 0.96875\n",
            "2018-12-31T06:43:37.773026: step 151, loss 0.216116, acc 0.953125\n",
            "2018-12-31T06:43:38.345190: step 152, loss 0.481629, acc 0.921875\n",
            "2018-12-31T06:43:38.909748: step 153, loss 0.164406, acc 1\n",
            "2018-12-31T06:43:39.471514: step 154, loss 0.23471, acc 0.984375\n",
            "2018-12-31T06:43:40.046842: step 155, loss 0.245886, acc 0.953125\n",
            "2018-12-31T06:43:40.606763: step 156, loss 0.292683, acc 0.984375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-01a117cd1bbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-01a117cd1bbe>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m     83\u001b[0m             _, step, summaries, loss, accuracy = sess.run(\n\u001b[1;32m     84\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "G65jic13-_QN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with tf.Graph().as_default():\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        cnn = textcnn(sequence_length=x_train.shape[1],\n",
        "                      num_classes=y_train.shape[1],\n",
        "                      vocab_size=vocab_size,\n",
        "                      embedding_size=FLAGS.embedding_dim,\n",
        "                      filter_sizes=list(map(int, FLAGS.filter_size.split(\",\"))),\n",
        "                      num_filters=FLAGS.num_filters,\n",
        "                      l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
        "\n",
        "        # Define Training procedure\n",
        "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
        "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "        # Keep track of gradient values and sparsity (optional)\n",
        "        grad_summaries = []\n",
        "        for g, v in grads_and_vars:\n",
        "            if g is not None:\n",
        "                grad_hist_summary = tf.summary.histogram(\"{}\".format(v.name), g)\n",
        "                sparsity_summary = tf.summary.scalar(\"{}\".format(v.name), tf.nn.zero_fraction(g))\n",
        "                grad_summaries.append(grad_hist_summary)\n",
        "                grad_summaries.append(sparsity_summary)\n",
        "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
        "\n",
        "        # Output directory for models and summaries\n",
        "        timestamp = str(int(time.time()))\n",
        "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "        print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "        # Summaries for loss and accuracy\n",
        "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
        "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
        "\n",
        "        # Train Summaries\n",
        "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
        "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "        # Dev summaries\n",
        "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
        "\n",
        "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
        "\n",
        "        # Initialize all variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        def train_step(x_batch, y_batch):\n",
        "            \"\"\"\n",
        "            A single training step\n",
        "            \"\"\"\n",
        "            feed_dict = {\n",
        "                cnn.input_x: x_batch,\n",
        "                cnn.input_y: y_batch,\n",
        "                cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
        "            }\n",
        "            _, step, summaries, loss, accuracy = sess.run(\n",
        "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            train_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "\n",
        "        def dev_step(x_batch, y_batch, writer=None):\n",
        "            \"\"\"\n",
        "            Evaluates model on a dev set\n",
        "            \"\"\"\n",
        "            feed_dict = {\n",
        "                cnn.input_x: x_batch,\n",
        "                cnn.input_y: y_batch,\n",
        "                cnn.dropout_keep_prob: 1.0\n",
        "            }\n",
        "            step, summaries, loss, accuracy = sess.run(\n",
        "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            if writer:\n",
        "                writer.add_summary(summaries, step)\n",
        "\n",
        "\n",
        "        def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
        "            \"\"\"\n",
        "            Generates a batch iterator for a dataset.\n",
        "            \"\"\"\n",
        "            data = np.array(data)\n",
        "            data_size = len(data)\n",
        "            num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
        "            for epoch in range(num_epochs):\n",
        "                # Shuffle the data at each epoch\n",
        "                if shuffle:\n",
        "                    shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "                    shuffled_data = data[shuffle_indices]\n",
        "                else:\n",
        "                    shuffled_data = data\n",
        "                for batch_num in range(num_batches_per_epoch):\n",
        "                    start_index = batch_num * batch_size\n",
        "                    end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "                    yield shuffled_data[start_index:end_index]\n",
        "\n",
        "\n",
        "        # Generate batches\n",
        "        batches = batch_iter(\n",
        "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
        "\n",
        "        testpoint = 0\n",
        "        # Training loop. For each batch...\n",
        "        for batch in batches:\n",
        "            x_batch, y_batch = zip(*batch)\n",
        "            train_step(x_batch, y_batch)\n",
        "            current_step = tf.train.global_step(sess, global_step)\n",
        "            if current_step % FLAGS.evaluate_every == 0:\n",
        "                if testpoint + 100 < len(x_test):\n",
        "                    testpoint += 100\n",
        "                else:\n",
        "                    testpoint = 0\n",
        "                print(\"\\nEvaluation:\")\n",
        "                dev_step(x_test[testpoint:testpoint+100], y_test[testpoint:testpoint+100], writer=dev_summary_writer)\n",
        "                print(\"\")\n",
        "            if current_step % FLAGS.checkpoint_every == 0:\n",
        "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "                print(\"Saved model checkpoint to {}\\n\".format(path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HZEayn9V7mQv",
        "colab_type": "code",
        "outputId": "75cc2cab-6c94-431c-cad8-cd58de75cc1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5601, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "6xAuUHpx-Qrp",
        "colab_type": "code",
        "outputId": "4bc2558b-8a05-48c6-d2c6-8f480fe1fab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x.shape[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "4UVVFPFA3Hvz",
        "colab_type": "code",
        "outputId": "cdde99f4-872a-4b8c-c673-4865b7c812c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nf6LKkyN24-z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRsfaP1G3GxS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}